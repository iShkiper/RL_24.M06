{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Методы ДП для поиска оптимальной стратегии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим на практике работу методов ДП для поиска оптимальной стратегии, которые обсуждались в лекциях."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Алгоритм итерации по стратегиям\n",
    "\n",
    "Реализуем метод итерации по стратегиям для поиска оптимальной стратегии. \n",
    "Этот метод представляет собой итерацию следующих двух шагов:\n",
    "\n",
    "1. оценка стратегии, путём вычисления её функции ценности состояний $v_{\\pi}(s)$,\n",
    "2. улучшение стратегии.\n",
    "\n",
    "Шаг оценки стратегии у нас уже реализован в коде, это метод `PolicyEvaluation()`. Сделаем лишь одну модификацию кода этого метода, которая позволит устанавливать начальную оценку функции ценности состояний $v_{\\pi}(s)$ вручную. Это удобно с той точки зрения, что для обновлённой стратегии начальную оценку функции ценности состояний можно инициализировать по найденной оценке этой фукнции для старой стратегии. Это ускоряет сходимость.\n",
    "\n",
    "Далее, приведём код для шага улучшения стратегии `PolicyImprovement()`. И напишем метод `IterationByPolicy()`, реализующий алгортим итерации по стратегиям.\n",
    "\n",
    "Также для удобства, напишем метод `GetOptimalActionsByPolicy()` для получения индексов оптимальных действий из найденной стратегии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оценка стратегии (в параметрах можно передать V, то есть фукнцию ценности старой стратегии)\n",
    "def PolicyEvaluation(env, policy, V, gamma, theta=1e-4):\n",
    "    nS = env.observation_space.n # количество различных состояний    \n",
    "    iteration = 0\n",
    "    while True:\n",
    "        delta = 0\n",
    "        # Вычисление новых оценок для всех состояний\n",
    "        for state in range(nS):         \n",
    "            v_s_old = V[state] # запоминаем значение v(s), для его сравнения с обновлённым значением v(s)\n",
    "            v_s = 0 # для обновлённого значения v(s)         \n",
    "            act_probs = policy[state]\n",
    "            for act, act_prob in enumerate(act_probs):\n",
    "                for next_sr in env.P[state][act]:\n",
    "                    trans_prob, new_state, reward, _ = next_sr\n",
    "                    v_s += act_prob * trans_prob * (reward + gamma * V[new_state])\n",
    "            V[state] = v_s\n",
    "            # найдём разницу между старым и новым значением v(s)\n",
    "            delta = np.max([delta, np.abs(V[state] - v_s_old)])\n",
    "        iteration = iteration + 1\n",
    "        # Критерий завершения итераций\n",
    "        if delta < theta:\n",
    "            break\n",
    "        if iteration > 2000:\n",
    "            print(\"Более 2000 итераций. Стоп.\")\n",
    "            return V\n",
    "    print(\"Число итераций: \", iteration)\n",
    "    return V\n",
    "\n",
    "# Улучшение стратегии\n",
    "def PolicyImprovement(env, oldPolicy, V, gamma):\n",
    "    nS = env.observation_space.n\n",
    "    nA = env.action_space.n\n",
    "    # пустой список из нулей для формирования обновлённой стратегии\n",
    "    newPolicy = np.zeros((nS,nA))\n",
    "    policy_changed = False\n",
    "    for state in range(nS):\n",
    "        q_s = [] # массив для значений q(s,a)\n",
    "        for act in range(nA):\n",
    "            q_s_a = 0 # для вычисления q(s,a)\n",
    "            for next_sr in env.P[state][act]:\n",
    "                trans_prob, new_state, reward, _ = next_sr\n",
    "                q_s_a += trans_prob * (reward + gamma * V[new_state])\n",
    "            q_s.append(q_s_a)\n",
    "        # ищем максимальные значения, формируем новую стратегию\n",
    "        mask = q_s == np.max(q_s)\n",
    "        newPolicy[state] = mask/np.sum(mask)\n",
    "    \n",
    "        # np.allclose сравнение числовых массивов с точностью до некоторого допуска\n",
    "        if not np.allclose(newPolicy[state], oldPolicy[state]):\n",
    "            policy_changed = True # флаг о том, что стратегия изменилась\n",
    "    return newPolicy, policy_changed\n",
    "\n",
    "\n",
    "def IterationByPolicy(env, gamma = 0.9, theta = 1e-4):\n",
    "    nS = env.observation_space.n\n",
    "    nA = env.action_space.n \n",
    "    # инициализация стратегией равновероятного выбора действий\n",
    "    policy = np.ones([nS, nA]) / nA\n",
    "    V = np.zeros(nS)\n",
    "    iteration = 0\n",
    "    while True:\n",
    "        # шаг оценки\n",
    "        V = PolicyEvaluation(env, policy, V, gamma, theta)\n",
    "        # шаг улучшения\n",
    "        policy, changed = PolicyImprovement(env, policy, V, gamma)\n",
    "\n",
    "        iteration = iteration + 1\n",
    "        if not changed: #завершить, если нет изменений в стратегии\n",
    "            break\n",
    "        if iteration > 1000:\n",
    "            print(\"Больше 1000 итераций. Стоп.\")\n",
    "            break\n",
    "            \n",
    "    print(\"Число шагов улучшения стратегии: \", iteration)\n",
    "    optPolicy = policy\n",
    "    V_optimal = PolicyEvaluation(env, optPolicy, V, gamma, theta)\n",
    "    return optPolicy, V_optimal\n",
    "\n",
    "\n",
    "# Получение оптимальных действий по оптимальной функции ценности состояний\n",
    "def GetOptimalActionsByPolicy(optPolicy):\n",
    "    nS = len(optPolicy)\n",
    "    # пустой список из нулей для записи оптимальных действий\n",
    "    bestActs = [0]*nS\n",
    "    for state in range(nS):\n",
    "        bestActs[state] = np.argwhere(optPolicy[state] == np.max(optPolicy[state])).flatten().tolist()\n",
    "    return bestActs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Алгоритм итерации по ценности\n",
    "\n",
    "Реализуем метод итерации по ценности для поиска оптимальной стратегии. Напомним, что для этого используется уравнение оптимальности Беллмана и итерационный метод для решения этого уравнения.\n",
    "\n",
    "$V_{k+1}(s) = \\max\\limits_a \\sum\\limits_{r\\in {\\mathcal R}} \\sum\\limits_{s'\\in {\\mathcal S}} p(s',r|s,a) (r + \\gamma V_k(s'))$\n",
    "\n",
    "Когда оптимальные значения ценностей $V^*$ найдены, то для определения оптимальной стратегии надо в каждом состоянии выбрать действия, приносящий максимальный доход, а именно:\n",
    "\n",
    "\n",
    "$\\pi^*(s) = \\underset{a}{\\mathrm{argmin}} \\sum\\limits_{r\\in {\\mathcal R}} \\sum\\limits_{s'\\in {\\mathcal S}} p(s',r|s,a) (r + \\gamma V_k(s'))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача: реализовать алгоритм итерации по ценности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IterationByValues(env, gamma, theta = 1e-4):\n",
    "    nS = env.observation_space.n\n",
    "    nA = env.action_space.n \n",
    "    V = np.zeros(nS)\n",
    "    iteration = 0\n",
    "    while True:\n",
    "        \n",
    "        ???\n",
    "        ???\n",
    "        ???\n",
    "        \n",
    "        if iteration > 2000:\n",
    "            print(\"Больше 2000 итераций. Стоп.\")\n",
    "            return V\n",
    "    print(\"Число итераций: \", iteration)\n",
    "    return V\n",
    "\n",
    "\n",
    "# Получение оптимальной стратегии по оптимальной функции ценности состояний\n",
    "def GetOptimalPolicyByV(env, OptValues, gamma):\n",
    "    nS = env.observation_space.n\n",
    "    nA = env.action_space.n \n",
    "    # пустой список из нулей для записи оптимальных действий\n",
    "    optPolicy = np.zeros((nS,nA))\n",
    "\n",
    "        ???\n",
    "        ???\n",
    "        ???\n",
    "\n",
    "    return optPolicy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2obRQEcUCYg2"
   },
   "source": [
    "### Пример 1\n",
    "\n",
    "Рассмотрим задачу о сеточном мире размера 4 на 4 клетки из книги Р. Саттона и Э. Барто \"Обучение с подкреплением\" и найдём оптимальную стратегию действий."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "# подгрузка среды\n",
    "from custom.env.gridworld import GridWorld4by4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Число итераций:  141\n",
      "Число итераций:  3\n",
      "Число итераций:  1\n",
      "Число шагов улучшения стратегии:  3\n",
      "Число итераций:  1\n",
      "Оптимальная стратегия: \n",
      " [[0.25 0.25 0.25 0.25]\n",
      " [1.   0.   0.   0.  ]\n",
      " [1.   0.   0.   0.  ]\n",
      " [0.5  0.5  0.   0.  ]\n",
      " [0.   0.   0.   1.  ]\n",
      " [0.5  0.   0.   0.5 ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.   0.   0.   1.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.   0.5  0.5  0.  ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.   0.   0.5  0.5 ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.25 0.25 0.25 0.25]]\n"
     ]
    }
   ],
   "source": [
    "gamma = 1.0\n",
    "env = GridWorld4by4()\n",
    "optPolicy, optVal = IterationByPolicy(env, gamma, theta=1e-5)\n",
    "np.round(optVal,3).reshape(4,4)\n",
    "print(\"Оптимальная стратегия: \\n\",optPolicy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0., -1., -2., -3.],\n",
       "       [-1., -2., -3., -2.],\n",
       "       [-2., -3., -2., -1.],\n",
       "       [-3., -2., -1.,  0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# оптимальные ценности действий\n",
    "optVal.reshape(4,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как было отмечено ранее, выбранная система вознаграждений ведёт к тому, что максимизация дохода в данной задаче предполагает как можно более быстрое движение в заключительные состояния из клеток сеточного мира. А ценности состояний будут означать среднее число шагов из этого состояния до заключительного состояния с обратным знаком при выбранной стратегии. По ответу ясно, что оптимальные значения найдены.\n",
    "\n",
    "Выведем также оптимальные действия в каждом состоянии:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[0, 1, 2, 3], [0], [0], [0, 1]],\n",
       " [[3], [0, 3], [0, 1, 2, 3], [1]],\n",
       " [[3], [0, 1, 2, 3], [1, 2], [1]],\n",
       " [[2, 3], [2], [2], [0, 1, 2, 3]]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optActs = GetOptimalActionsByPolicy(optPolicy)\n",
    "optActs = [optActs[x:x+4] for x in range(0, 16, 4)] \n",
    "optActs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее, найдём оптимальные значения методом итерации по ценности. Результат одинаковый, но в данном случае сходимости гораздо более быстрая."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Число итераций:  4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0., -1., -2., -3.],\n",
       "       [-1., -2., -3., -2.],\n",
       "       [-2., -3., -2., -1.],\n",
       "       [-3., -2., -1.,  0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma = 1.0\n",
    "optVal=IterationByValues(env, gamma, theta=1e-6)\n",
    "np.round(optVal,3).reshape(4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Оптимальная стратегия: \n",
      " [[0.25 0.25 0.25 0.25]\n",
      " [1.   0.   0.   0.  ]\n",
      " [1.   0.   0.   0.  ]\n",
      " [0.5  0.5  0.   0.  ]\n",
      " [0.   0.   0.   1.  ]\n",
      " [0.5  0.   0.   0.5 ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.   0.   0.   1.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.   0.5  0.5  0.  ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.   0.   0.5  0.5 ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.25 0.25 0.25 0.25]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Оптимальная стратегия: \\n\", GetOptimalPolicyByV(env,optVal, gamma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[0, 1, 2, 3], [0], [0], [0, 1]],\n",
       " [[3], [0, 3], [0, 1, 2, 3], [1]],\n",
       " [[3], [0, 1, 2, 3], [1, 2], [1]],\n",
       " [[2, 3], [2], [2], [0, 1, 2, 3]]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optPolicy = GetOptimalPolicyByV(env,optVal, gamma)\n",
    "optActs = GetOptimalActionsByPolicy(optPolicy)\n",
    "optActs = [optActs[x:x+4] for x in range(0, env.observation_space.n, 4)] \n",
    "optActs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример 2\n",
    "\n",
    "Далее, применим реализованные алгоритмы для решения задачи о замёрзшем озере FrozenLake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пространство состояний: Discrete(16)\n",
      "Пространство действий: Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\") #создание экземпляра среды\n",
    "env.reset() # инициализация среды\n",
    "\n",
    "# выведем описания пространств состояний и действий\n",
    "print(\"Пространство состояний:\", env.observation_space)\n",
    "print(\"Пространство действий:\", env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустим симуляцию со случайными действиями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", render_mode='human').unwrapped\n",
    "env.reset()\n",
    "\n",
    "while True:\n",
    "    time.sleep(.2)\n",
    "    state, reward, terminated, truncated, _ = env.step(env.action_space.sample()) # выбрать действие случайно\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустим алгоритм поиска оптимальной стратегии по методу итерации по стратегиям."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Число итераций:  31\n",
      "Число итераций:  198\n",
      "Число итераций:  179\n",
      "Число шагов улучшения стратегии:  3\n",
      "Число итераций:  1\n",
      "[[0.823 0.823 0.823 0.823]\n",
      " [0.823 0.    0.529 0.   ]\n",
      " [0.823 0.823 0.765 0.   ]\n",
      " [0.    0.882 0.941 0.   ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 1.  ],\n",
       "       [0.  , 0.  , 0.  , 1.  ],\n",
       "       [0.  , 0.  , 0.  , 1.  ],\n",
       "       [1.  , 0.  , 0.  , 0.  ],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.5 , 0.  , 0.5 , 0.  ],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.  , 0.  , 0.  , 1.  ],\n",
       "       [0.  , 1.  , 0.  , 0.  ],\n",
       "       [1.  , 0.  , 0.  , 0.  ],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.  , 0.  , 1.  , 0.  ],\n",
       "       [0.  , 1.  , 0.  , 0.  ],\n",
       "       [0.25, 0.25, 0.25, 0.25]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optPolicy, optVal = IterationByPolicy(env, gamma=1.0, theta=1e-5)\n",
    "print(np.round(optVal,3).reshape(4,4))\n",
    "optPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[0], [3], [3], [3]],\n",
       " [[0], [0, 1, 2, 3], [0, 2], [0, 1, 2, 3]],\n",
       " [[3], [1], [0], [0, 1, 2, 3]],\n",
       " [[0, 1, 2, 3], [2], [1], [0, 1, 2, 3]]]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optActs = GetOptimalActionsByPolicy(optPolicy)\n",
    "nS = len(env.P)\n",
    "optActs_show = [ optActs[x:x+4] for x in range(0, nS, 4) ] \n",
    "optActs_show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напомним, что нумерация действий значит следующее:\n",
    "\n",
    "    0: влево\n",
    "    1: вниз\n",
    "    2: вправо\n",
    "    3: вверх\n",
    "    \n",
    "Аналогично, применим метод итерации по ценности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Число итераций:  254\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.82336, 0.82331, 0.82327, 0.82325],\n",
       "       [0.82338, 0.     , 0.5293 , 0.     ],\n",
       "       [0.8234 , 0.82344, 0.76463, 0.     ],\n",
       "       [0.     , 0.88229, 0.94114, 0.     ]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma = 1.0\n",
    "optVal=IterationByValues(env, gamma, theta=1e-5)\n",
    "np.round(optVal,5).reshape(4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[0], [3], [3], [3]],\n",
       " [[0], [0, 1, 2, 3], [0, 2], [0, 1, 2, 3]],\n",
       " [[3], [1], [0], [0, 1, 2, 3]],\n",
       " [[0, 1, 2, 3], [2], [1], [0, 1, 2, 3]]]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optPolicy = GetOptimalPolicyByV(env,optVal, gamma)\n",
    "optActs = GetOptimalActionsByPolicy(optPolicy)\n",
    "nS = len(env.P)\n",
    "optActs_show = [ optActs[x:x+4] for x in range(0, nS, 4) ] \n",
    "optActs_show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 1.  ],\n",
       "       [0.  , 0.  , 0.  , 1.  ],\n",
       "       [0.  , 0.  , 0.  , 1.  ],\n",
       "       [1.  , 0.  , 0.  , 0.  ],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.5 , 0.  , 0.5 , 0.  ],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.  , 0.  , 0.  , 1.  ],\n",
       "       [0.  , 1.  , 0.  , 0.  ],\n",
       "       [1.  , 0.  , 0.  , 0.  ],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.  , 0.  , 1.  , 0.  ],\n",
       "       [0.  , 1.  , 0.  , 0.  ],\n",
       "       [0.25, 0.25, 0.25, 0.25]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optPolicy = GetOptimalPolicyByV(env,optVal, gamma)\n",
    "optPolicy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустим симуляцию среды с обученным агентом, который действует по оптимальной стратегии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Up)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", render_mode='ansi')\n",
    "nA = env.action_space.n \n",
    "\n",
    "state, _ = env.reset()\n",
    "print(env.render())\n",
    "\n",
    "while True:\n",
    "    # выбор действий в соответствии с вероятностями из оптимальной стратегии\n",
    "    action = np.random.choice(nA, p = optPolicy[state]) \n",
    "    state, reward, terminated, truncated, _ = env.step(action)\n",
    "    \n",
    "    time.sleep(.2)\n",
    "    clear_output(wait=True)\n",
    "    print(env.render())\n",
    "\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "    \n",
    "print(\"finish\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведём симуляцию на 1000 эпизодах и найдём число успешных эпизодов при оптимальной стратегии и стратегии равновероятного выбора действия."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(env, n_episodes, optPolicy):\n",
    "    for episode in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        while True:\n",
    "            # выбираем действия по стратегии\n",
    "            action = np.random.choice(nA, p = optPolicy[state]) \n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            if terminated or truncated:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посколько параметр `gamma=1.0`, то нет штрафа за длительное взаимодействие со средой. Для текущей задачи это значит, что агент обучается находить максимально безопасный путь. Поэтому временное ограничение в 100 шагов в среде FrozenLake может превышаться. Для изменения этого временного ограничения можно при создании среды указать `max_episode_steps=1000`. В этом случае временное ограничение будет изменено на 1000 шагов. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Число успешных завершений эпизода:  0.0\n"
     ]
    }
   ],
   "source": [
    "n_episodes = 1000\n",
    "env = gym.make(\"FrozenLake-v1\", render_mode='ansi', max_episode_steps=1000)\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env, buffer_length=n_episodes)\n",
    "\n",
    "play(env, n_episodes, optPolicy)\n",
    "print(\"Число успешных завершений эпизода: \", sum(env.return_queue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Число успешных завершений эпизода:  13.0\n"
     ]
    }
   ],
   "source": [
    "# равновероятный выбор действия\n",
    "env.reset()\n",
    "rand_policy = np.ones([16, 4]) / 4\n",
    "play(env, n_episodes, rand_policy)\n",
    "print(\"Число успешных завершений эпизода: \", sum(env.return_queue))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В среднем около 82% эпизодов удалось завершить успешно. А при стратегии равновероятного выбора завершить эпизод удаётся лишь в среднем при 1.4% случаях.\n",
    "\n",
    "Можно также найти решения при различных значениях `gamma`. Выше решение найдено при `gamma=1`. Это значит поиск стратегии, которая прокладывает максимально безопасный путь.\n",
    "\n",
    "Можно также посчитать решение, например, при `gamma=0.9`. Тогда агент будет искать баланс между безопасным путём и длительностью прохождения (в данном случае далёкая награда за успешное завершение будет сильно обесценена). Полученная оптимальная стратегия будет отличаться от оптимальной стратегии при `gamma=1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Число итераций:  21\n",
      "Число итераций:  46\n",
      "Число шагов улучшения стратегии:  2\n",
      "Число итераций:  1\n",
      "[[0.069 0.061 0.074 0.056]\n",
      " [0.092 0.    0.112 0.   ]\n",
      " [0.145 0.247 0.3   0.   ]\n",
      " [0.    0.38  0.639 0.   ]]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", render_mode='ansi').unwrapped\n",
    "\n",
    "optPolicy, optVal = IterationByPolicy(env, gamma=0.9, theta=1e-5)\n",
    "print(np.round(optVal,3).reshape(4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[0], [3], [0], [3]],\n",
       " [[0], [0, 1, 2, 3], [0, 2], [0, 1, 2, 3]],\n",
       " [[3], [1], [0], [0, 1, 2, 3]],\n",
       " [[0, 1, 2, 3], [2], [1], [0, 1, 2, 3]]]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optActs = GetOptimalActionsByPolicy(optPolicy)\n",
    "nS = len(env.P)\n",
    "optActs_show = [ optActs[x:x+4] for x in range(0, nS, 4) ] \n",
    "optActs_show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Число успешных завершений эпизода:  780.0\n"
     ]
    }
   ],
   "source": [
    "n_episodes = 1000\n",
    "env = gym.make(\"FrozenLake-v1\", render_mode='ansi', max_episode_steps=1000)\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env, buffer_length=n_episodes)\n",
    "\n",
    "play(env, n_episodes, optPolicy)\n",
    "print(\"Число успешных завершений эпизода: \", sum(env.return_queue))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Среднее число успешных завершений теперь около 78%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример 3\n",
    "\n",
    "Рассмотрим задачу FrozenLake с полем 8 на 8 клеток. Правила аналогичны задаче FrozenLake с полем 4 на 4, просто поле имеет теперь больший размер."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пространство состояний: Discrete(64)\n",
      "Пространство действий: Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", map_name=\"8x8\")\n",
    "env.reset() # инициализация среды\n",
    "\n",
    "# выведем описания пространств состояний и действий\n",
    "print(\"Пространство состояний:\", env.observation_space)\n",
    "print(\"Пространство действий:\", env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустим симуляцию среды со случайными действиями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", map_name=\"8x8\", render_mode = 'human').unwrapped\n",
    "env.reset()\n",
    "\n",
    "while True:\n",
    "    time.sleep(.2)\n",
    "    state, reward, terminated, truncated, _ = env.step(env.action_space.sample()) # выбрать действие случайно\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как и ранее, сначала применим метод итерации по стратегиям."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Число итераций:  14\n",
      "Число итераций:  118\n",
      "Число итераций:  67\n",
      "Число итераций:  50\n",
      "Число итераций:  42\n",
      "Число итераций:  29\n",
      "Число итераций:  4\n",
      "Число итераций:  1\n",
      "Число шагов улучшения стратегии:  8\n",
      "Число итераций:  1\n",
      "[[1.   1.   1.   1.   1.   1.   1.   1.  ]\n",
      " [1.   1.   1.   1.   1.   1.   1.   1.  ]\n",
      " [0.99 0.97 0.92 0.   0.85 0.94 0.98 1.  ]\n",
      " [0.98 0.92 0.79 0.47 0.62 0.   0.94 1.  ]\n",
      " [0.97 0.81 0.53 0.   0.54 0.61 0.85 1.  ]\n",
      " [0.96 0.   0.   0.17 0.38 0.44 0.   1.  ]\n",
      " [0.96 0.   0.19 0.12 0.   0.33 0.   1.  ]\n",
      " [0.96 0.7  0.44 0.   0.28 0.55 0.78 0.  ]]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", map_name=\"8x8\").unwrapped\n",
    "env.reset()\n",
    "\n",
    "optPolicy, optVal = IterationByPolicy(env, gamma=1.0, theta=1e-3)\n",
    "print(np.round(optVal,2).reshape(8,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[1], [2], [2], [2], [2], [2], [2], [2]],\n",
       " [[3], [3], [3], [3], [3], [3], [3], [2]],\n",
       " [[0], [3], [0], [0, 1, 2, 3], [2], [3], [3], [2]],\n",
       " [[0], [0], [0], [1, 3], [0], [0, 1, 2, 3], [2], [2]],\n",
       " [[0], [3], [0, 3], [0, 1, 2, 3], [2], [1], [3], [2]],\n",
       " [[0], [0, 1, 2, 3], [0, 1, 2, 3], [1, 2], [3], [0], [0, 1, 2, 3], [2]],\n",
       " [[0], [0, 1, 2, 3], [1, 2], [0, 3], [0, 1, 2, 3], [0, 2], [0, 1, 2, 3], [2]],\n",
       " [[0], [1], [0], [0, 1, 2, 3], [1, 2], [2], [1], [0, 1, 2, 3]]]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optPolicy = GetOptimalPolicyByV(env, optVal, gamma = 1.0)\n",
    "optActs = GetOptimalActionsByPolicy(optPolicy)\n",
    "nS = len(env.P)\n",
    "optActs_show = [ optActs[x:x+8] for x in range(0, nS, 8) ] \n",
    "optActs_show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим также метод итерация по ценности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Число итераций:  343\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  ],\n",
       "       [1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  ],\n",
       "       [1.  , 0.98, 0.92, 0.  , 0.86, 0.95, 0.98, 1.  ],\n",
       "       [1.  , 0.93, 0.8 , 0.47, 0.62, 0.  , 0.94, 1.  ],\n",
       "       [1.  , 0.82, 0.54, 0.  , 0.54, 0.61, 0.85, 1.  ],\n",
       "       [1.  , 0.  , 0.  , 0.17, 0.38, 0.44, 0.  , 1.  ],\n",
       "       [1.  , 0.  , 0.19, 0.12, 0.  , 0.33, 0.  , 1.  ],\n",
       "       [0.99, 0.73, 0.46, 0.  , 0.28, 0.55, 0.78, 0.  ]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma = 1.0\n",
    "optVal=IterationByValues(env, gamma, theta=1e-4)\n",
    "np.round(optVal,2).reshape(8,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "optPolicy = GetOptimalPolicyByV(env,optVal, gamma)\n",
    "optActs = GetOptimalActionsByPolicy(optPolicy)\n",
    "#optActs_show = [ optActs[x:x+8] for x in range(0, env.nS, 8) ] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустим симуляцию среды с обученным агентом, который действует по оптимальной стратегии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFF\u001b[41mG\u001b[0m\n",
      "\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", map_name=\"8x8\", render_mode='ansi', max_episode_steps=1000)\n",
    "\n",
    "nA = env.action_space.n \n",
    "state, _ = env.reset()\n",
    "print(env.render())\n",
    "\n",
    "while True:\n",
    "    # выбор действий в соответствии с вероятностями из оптимальной стратегии\n",
    "    action = np.random.choice(nA, p = optPolicy[state]) \n",
    "    state, reward, terminated, truncated, _ = env.step(action)\n",
    "    \n",
    "    time.sleep(.2)\n",
    "    clear_output(wait=True)\n",
    "    print(env.render())\n",
    "\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "    \n",
    "print(\"finish\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведём симуляцию на 1000 эпизодах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(env, n_episodes, optPolicy):\n",
    "    for episode in range(n_episodes):\n",
    "        nA = env.action_space.n \n",
    "        state, _ = env.reset()\n",
    "        while True:\n",
    "            # выбираем действия по стратегии\n",
    "            action = np.random.choice(nA, p = optPolicy[state]) \n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            if terminated or truncated:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Число успешных завершений эпизода:  100.0\n"
     ]
    }
   ],
   "source": [
    "n_episodes = 100\n",
    "env = gym.make(\"FrozenLake-v1\", map_name=\"8x8\", render_mode='ansi', max_episode_steps=1000)\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env, buffer_length=n_episodes)\n",
    "\n",
    "play(env, n_episodes, optPolicy)\n",
    "print(\"Число успешных завершений эпизода: \", sum(env.return_queue))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То есть в этой задаче оптимальная стратегия всегда позволяет добраться до целевой клетки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример 4 \n",
    "\n",
    "Рассмотрим задачу об управлении такси из библиотеки `gymnasium`. Эта среда также имеет полное описание динамики с помощью словаря `env.P`.\n",
    "\n",
    "Среда представляет собой клеточное поле в виде квадрата 5 на 5 клеток, на котором отмечены 4 клетки R, G, Y, B (сокращения от названий цветов). В одной из этих клеток, выбранной случайным образом, находится пассажир. И в одной и трёх оставшихся клеток находится место, куда пассажир хочет добраться на такси.\n",
    "\n",
    "Такси может находится в одной из 25 клеток, пассажир может быть в такси или в одной из 4-х выделенных клеток и точка назначения может быть в одной из выделенных клеток. Эти три параметра задают весь набор состояний. То есть общее число состояний равно $25 *5 *4=500.$\n",
    "Таким образом, состояние среды представляет собой индекс 1 до 500.\n",
    "\n",
    "Есть 6 возможных действий: \n",
    "\n",
    "    движение такси вверх, вниз, влево, вправо, а также подобрать пассажира и высадить пассажира.\n",
    "\t\n",
    "Возможные вознаграждения следующие:\n",
    "\n",
    "    -1 за временной шаг, \n",
    "    +20 за успешную посадку\\высадку, \n",
    "    -10 за посадку\\высадку в неправильных клетках.\n",
    "\n",
    "Эпизод заканчивается, когда пассажир высажен в нужной клетке, либо время взаимодействия превышает 200 временных шагов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пространство состояний: Discrete(500)\n",
      "Пространство действий: Discrete(6)\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "env = gym.make(\"Taxi-v3\")\n",
    "env.reset()\n",
    "\n",
    "# выведем описания пространств состояний и действий\n",
    "print(\"Пространство состояний:\", env.observation_space)\n",
    "print(\"Пространство действий:\", env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустим симуляцию со случайными действями. Опять же доступно два варианта визуализации: графическая и текстовая."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "env = gym.make(\"Taxi-v3\", render_mode = 'human', max_episode_steps = 100)\n",
    "env.reset()\n",
    "\n",
    "while True:   \n",
    "    time.sleep(.1)\n",
    "    state, reward, terminated, truncated, _ = env.step(env.action_space.sample()) # выбрать действие случайно\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[43mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "env = gym.make(\"Taxi-v3\", render_mode='ansi', max_episode_steps = 100)\n",
    "env.reset()\n",
    "print(env.render())\n",
    "\n",
    "while True:       \n",
    "    state, reward, terminated, truncated, _ = env.step(env.action_space.sample()) # выбрать действие случайно\n",
    "    \n",
    "    time.sleep(.1)\n",
    "    clear_output(wait=True)      \n",
    "    print(env.render())\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим метод итерации по стратегиям для обучения агента оптимальной стратегии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Число итераций:  134\n",
      "Число итераций:  155\n",
      "Число итераций:  58\n",
      "Число итераций:  6\n",
      "Число итераций:  85\n",
      "Число итераций:  45\n",
      "Число итераций:  32\n",
      "Число итераций:  54\n",
      "Число итераций:  30\n",
      "Число итераций:  31\n",
      "Число итераций:  2\n",
      "Число шагов улучшения стратегии:  11\n",
      "Число итераций:  1\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Taxi-v3\", render_mode='ansi').unwrapped\n",
    "env.reset()\n",
    "\n",
    "optPolicy, optVal = IterationByPolicy(env, gamma=0.95, theta=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optActs = GetOptimalActionsByPolicy(optPolicy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим метод итерации по ценности для обучения агента оптимальной стратегии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Число итераций:  146\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.95\n",
    "optVal=IterationByValues(env, gamma, theta=1e-5)\n",
    "#np.round(optVal,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0. , 0. , 0. , 0. , 1. , 0. ],\n",
       "       [0. , 0. , 0. , 0. , 1. , 0. ],\n",
       "       [0. , 0. , 0. , 0. , 1. , 0. ],\n",
       "       ...,\n",
       "       [0. , 1. , 0. , 0. , 0. , 0. ],\n",
       "       [0. , 0.5, 0. , 0.5, 0. , 0. ],\n",
       "       [0. , 0. , 0. , 1. , 0. , 0. ]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optPolicy = GetOptimalPolicyByV(env,optVal, gamma)\n",
    "optPolicy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустим симуляцию среды с обученным агентом, который действует по оптимальной стратегии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "env = gym.make(\"Taxi-v3\", render_mode = 'ansi')\n",
    "nA = env.action_space.n \n",
    "\n",
    "state, _ = env.reset()\n",
    "print(env.render())\n",
    "\n",
    "while True:\n",
    "    # выбор действий в соответствии с вероятностями из оптимальной стратегии\n",
    "    action = np.random.choice(nA, p = optPolicy[state]) \n",
    "    state, reward, terminated, truncated, _ = env.step(action)\n",
    "    \n",
    "    time.sleep(.2)\n",
    "    clear_output(wait=True)\n",
    "    print(env.render())\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        break\n",
    "    \n",
    "print(\"finish\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    }
   ],
   "source": [
    "# симуляция с рендерингом\n",
    "env = gym.make(\"Taxi-v3\", render_mode = 'human')\n",
    "nA = env.action_space.n \n",
    "\n",
    "state, _ = env.reset()\n",
    "\n",
    "while True:\n",
    "    # выбор действий в соответствии с вероятностями из оптимальной стратегии\n",
    "    action = np.random.choice(nA, p = optPolicy[state]) \n",
    "    state, reward, terminated, truncated, _ = env.step(action)\n",
    "    if terminated or truncated:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведём также симуляцию на большом числе эпизодов. Будем записывать полученные доходы и выведем гистограмму распределения доходов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(env, n_episodes, optPolicy):\n",
    "    for episode in range(n_episodes):\n",
    "        nA = env.action_space.n \n",
    "        state, _ = env.reset()\n",
    "        while True:\n",
    "            # выбираем действия по стратегии\n",
    "            action = np.random.choice(nA, p = optPolicy[state]) \n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            if terminated or truncated:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 10000\n",
    "env = gym.make(\"Taxi-v3\")\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env, buffer_length=n_episodes)\n",
    "\n",
    "play(env, n_episodes, optPolicy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Средний доход за эпизод равен: 7.9479\n",
      "Минимальный доход за эпизод равен: 3.0\n",
      "Максимальный доход за эпизод равен: 15.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjEAAAGdCAYAAADjWSL8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAnCElEQVR4nO3df1Dc9Z3H8dfKwgoMbAIJu+4VLM4wGgO1Kelh0GvihBCthHF6LbEo5mpG04kmriEJ4azX1BnhSGtiK2M0jnN4SXP4j+S8ThqDvRzKYRIK0po0NXXKRCJZ8a5kgQQB4Xt/OH6vC/lFXLL7Ic/HzHem38/3/f3m/f0O03352e/3uw7LsiwBAAAY5ppINwAAAHA5CDEAAMBIhBgAAGAkQgwAADASIQYAABiJEAMAAIxEiAEAAEYixAAAACM5I93AVBkbG1N3d7eSkpLkcDgi3Q4AALgElmWpv79fPp9P11xz4bmWaRtiuru7lZ6eHuk2AADAZejq6tJXvvKVC9ZM2xCTlJQk6fOLkJycHOFuAADApejr61N6err9OX4h0zbEfPEVUnJyMiEGAADDXMqtINzYCwAAjESIAQAARiLEAAAAIxFiAACAkQgxAADASIQYAABgJEIMAAAwEiEGAAAYiRADAACMRIgBAABGIsQAAAAjEWIAAICRCDEAAMBIhBgAAGAkZ6QbAKY7h2NGpFuQJFnW6Ui3AABhxUwMAAAwEiEGAAAYiRADAACMRIgBAABGIsQAAAAjEWIAAICRCDEAAMBIhBgAAGAkXnYHXCWi5aV7Ei/eAxAezMQAAAAjEWIAAICRCDEAAMBIkw4xb731lpYtWyafzyeHw6E9e/act3bVqlVyOBx69tlnQ8aHhoa0Zs0azZo1S4mJiSouLtbJkydDanp7e1VWVia32y23262ysjKdPn16su0CAIBpatIh5syZM7rllltUW1t7wbo9e/bo0KFD8vl8E7b5/X41NDSovr5ezc3NGhgYUFFRkUZHR+2a0tJSdXR0aN++fdq3b586OjpUVlY22XYBAMA0Nemnk+666y7dddddF6z56KOP9Oijj+qNN97Q3XffHbItGAzq5Zdf1s6dO1VQUCBJ2rVrl9LT0/Xmm29q6dKlOnbsmPbt26eDBw8qLy9PkvTSSy9pwYIFev/993XjjTdOtm0AADDNhP2emLGxMZWVlWnDhg2aO3fuhO1tbW0aGRlRYWGhPebz+ZSdna2WlhZJ0jvvvCO3220HGEm69dZb5Xa77ZrxhoaG1NfXF7IAAIDpK+whpqamRk6nU2vXrj3n9kAgoLi4OM2cOTNk3OPxKBAI2DVpaWkT9k1LS7Nrxquurrbvn3G73UpPT/+SZwIAAKJZWENMW1ubfv7zn6uurk4Oh2NS+1qWFbLPufYfX/PXKisrFQwG7aWrq2tyzQMAAKOENcS8/fbb6unpUUZGhpxOp5xOp06cOKHy8nJ99atflSR5vV4NDw+rt7c3ZN+enh55PB675uOPP55w/E8++cSuGc/lcik5OTlkAQAA01dYQ0xZWZl+//vfq6Ojw158Pp82bNigN954Q5KUm5ur2NhYNTY22vudOnVKR44cUX5+viRpwYIFCgaDOnz4sF1z6NAhBYNBuwYAAFzdJv100sDAgD744AN7vbOzUx0dHUpJSVFGRoZSU1ND6mNjY+X1eu0nitxut1auXKny8nKlpqYqJSVF69evV05Ojv200pw5c3TnnXfqoYce0osvvihJevjhh1VUVMSTSQAAQNJlhJjf/va3uuOOO+z1devWSZJWrFihurq6SzrGtm3b5HQ6VVJSosHBQS1evFh1dXWKiYmxa375y19q7dq19lNMxcXFF303DQAAuHo4LMuyIt3EVOjr65Pb7VYwGOT+GERUNP16dLTgV6wBnM9kPr/57SQAAGAkQgwAADASIQYAABiJEAMAAIxEiAEAAEYixAAAACMRYgAAgJEIMQAAwEiEGAAAYCRCDAAAMBIhBgAAGIkQAwAAjESIAQAARiLEAAAAIxFiAACAkQgxAADASIQYAABgJEIMAAAwEiEGAAAYiRADAACMRIgBAABGIsQAAAAjEWIAAICRCDEAAMBIhBgAAGAkQgwAADASIQYAABiJEAMAAIxEiAEAAEYixAAAACMRYgAAgJEIMQAAwEiEGAAAYCRCDAAAMBIhBgAAGMkZ6QYAXH0cjhmRbsFmWacj3QKAy8RMDAAAMNKkQ8xbb72lZcuWyefzyeFwaM+ePfa2kZERVVRUKCcnR4mJifL5fHrggQfU3d0dcoyhoSGtWbNGs2bNUmJiooqLi3Xy5MmQmt7eXpWVlcntdsvtdqusrEynT5++rJMEAADTz6RDzJkzZ3TLLbeotrZ2wrazZ8+qvb1dTz75pNrb2/Xaa6/p+PHjKi4uDqnz+/1qaGhQfX29mpubNTAwoKKiIo2Ojto1paWl6ujo0L59+7Rv3z51dHSorKzsMk4RAABMRw7LsqzL3tnhUENDg+65557z1rS2tupv//ZvdeLECWVkZCgYDGr27NnauXOnli9fLknq7u5Wenq69u7dq6VLl+rYsWO6+eabdfDgQeXl5UmSDh48qAULFuiPf/yjbrzxxov21tfXJ7fbrWAwqOTk5Ms9ReBLi6b7PzAR98QA0WUyn99Tfk9MMBiUw+HQjBkzJEltbW0aGRlRYWGhXePz+ZSdna2WlhZJ0jvvvCO3220HGEm69dZb5Xa77ZrxhoaG1NfXF7IAAIDpa0pDzKeffqpNmzaptLTUTlOBQEBxcXGaOXNmSK3H41EgELBr0tLSJhwvLS3Nrhmvurravn/G7XYrPT09zGcDAACiyZSFmJGREd17770aGxvT888/f9F6y7LkcDjs9b/+3+er+WuVlZUKBoP20tXVdfnNAwCAqDclIWZkZEQlJSXq7OxUY2NjyHdaXq9Xw8PD6u3tDdmnp6dHHo/Hrvn4448nHPeTTz6xa8ZzuVxKTk4OWQAAwPQV9hDzRYD505/+pDfffFOpqakh23NzcxUbG6vGxkZ77NSpUzpy5Ijy8/MlSQsWLFAwGNThw4ftmkOHDikYDNo1AADg6jbpN/YODAzogw8+sNc7OzvV0dGhlJQU+Xw+ffe731V7e7t+9atfaXR01L6HJSUlRXFxcXK73Vq5cqXKy8uVmpqqlJQUrV+/Xjk5OSooKJAkzZkzR3feeaceeughvfjii5Kkhx9+WEVFRZf0ZBIAAJj+Jv2I9X/913/pjjvumDC+YsUKbd68WZmZmefc78CBA1q0aJGkz2/43bBhg3bv3q3BwUEtXrxYzz//fMjNuH/5y1+0du1avf7665Kk4uJi1dbW2k85XQyPWCNa8Ih1dOMRayC6TObz+0u9JyaaEWIQLQgx0Y0QA0SXqHpPDAAAwFQgxAAAACMRYgAAgJEIMQAAwEiEGAAAYCRCDAAAMBIhBgAAGIkQAwAAjESIAQAARiLEAAAAI036ByABE/CqfwCY/piJAQAARiLEAAAAIxFiAACAkQgxAADASIQYAABgJEIMAAAwEiEGAAAYiRADAACMRIgBAABGIsQAAAAjEWIAAICRCDEAAMBIhBgAAGAkQgwAADASIQYAABiJEAMAAIxEiAEAAEYixAAAACMRYgAAgJEIMQAAwEiEGAAAYCRCDAAAMBIhBgAAGIkQAwAAjOSMdAMAEEkOx4xItyBJsqzTkW4BMA4zMQAAwEiTDjFvvfWWli1bJp/PJ4fDoT179oRstyxLmzdvls/nU3x8vBYtWqSjR4+G1AwNDWnNmjWaNWuWEhMTVVxcrJMnT4bU9Pb2qqysTG63W263W2VlZTp9+vSkTxAAAExPkw4xZ86c0S233KLa2tpzbt+yZYu2bt2q2tpatba2yuv1asmSJerv77dr/H6/GhoaVF9fr+bmZg0MDKioqEijo6N2TWlpqTo6OrRv3z7t27dPHR0dKisru4xTBAAA05HDsizrsnd2ONTQ0KB77rlH0uezMD6fT36/XxUVFZI+n3XxeDyqqanRqlWrFAwGNXv2bO3cuVPLly+XJHV3dys9PV179+7V0qVLdezYMd188806ePCg8vLyJEkHDx7UggUL9Mc//lE33njjRXvr6+uT2+1WMBhUcnLy5Z4iDBUt9zkAl4p7YoDPTebzO6z3xHR2dioQCKiwsNAec7lcWrhwoVpaWiRJbW1tGhkZCanx+XzKzs62a9555x253W47wEjSrbfeKrfbbdeMNzQ0pL6+vpAFAABMX2ENMYFAQJLk8XhCxj0ej70tEAgoLi5OM2fOvGBNWlrahOOnpaXZNeNVV1fb98+43W6lp6d/6fMBAADRa0qeTnI4HCHrlmVNGBtvfM256i90nMrKSgWDQXvp6uq6jM4BAIApwhpivF6vJE2YLenp6bFnZ7xer4aHh9Xb23vBmo8//njC8T/55JMJszxfcLlcSk5ODlkAAMD0FdYQk5mZKa/Xq8bGRntseHhYTU1Nys/PlyTl5uYqNjY2pObUqVM6cuSIXbNgwQIFg0EdPnzYrjl06JCCwaBdg+jjcMyImgUAMP1N+o29AwMD+uCDD+z1zs5OdXR0KCUlRRkZGfL7/aqqqlJWVpaysrJUVVWlhIQElZaWSpLcbrdWrlyp8vJypaamKiUlRevXr1dOTo4KCgokSXPmzNGdd96phx56SC+++KIk6eGHH1ZRUdElPZkEAACmv0mHmN/+9re644477PV169ZJklasWKG6ujpt3LhRg4ODWr16tXp7e5WXl6f9+/crKSnJ3mfbtm1yOp0qKSnR4OCgFi9erLq6OsXExNg1v/zlL7V27Vr7Kabi4uLzvpsGAABcfb7Ue2KiGe+JufL4Gge4fLwnBvhcxN4TAwAAcKUQYgAAgJEIMQAAwEiEGAAAYCRCDAAAMBIhBgAAGIkQAwAAjESIAQAARiLEAAAAIxFiAACAkQgxAADASIQYAABgJEIMAAAwEiEGAAAYiRADAACMRIgBAABGIsQAAAAjEWIAAICRCDEAAMBIhBgAAGAkQgwAADASIQYAABiJEAMAAIxEiAEAAEYixAAAACMRYgAAgJEIMQAAwEiEGAAAYCRCDAAAMBIhBgAAGIkQAwAAjESIAQAARiLEAAAAIxFiAACAkQgxAADASM5INwAAkByOGZFuwWZZpyPdAnBJmIkBAABGCnuI+eyzz/SjH/1ImZmZio+P1w033KCnnnpKY2Njdo1lWdq8ebN8Pp/i4+O1aNEiHT16NOQ4Q0NDWrNmjWbNmqXExEQVFxfr5MmT4W4XAAAYKuwhpqamRi+88IJqa2t17NgxbdmyRT/96U/13HPP2TVbtmzR1q1bVVtbq9bWVnm9Xi1ZskT9/f12jd/vV0NDg+rr69Xc3KyBgQEVFRVpdHQ03C0DAAADOSzLssJ5wKKiInk8Hr388sv22N///d8rISFBO3fulGVZ8vl88vv9qqiokPT5rIvH41FNTY1WrVqlYDCo2bNna+fOnVq+fLkkqbu7W+np6dq7d6+WLl160T76+vrkdrsVDAaVnJwczlPEeUTTd/oALh/3xCCSJvP5HfaZmNtvv12/+c1vdPz4cUnS7373OzU3N+vb3/62JKmzs1OBQECFhYX2Pi6XSwsXLlRLS4skqa2tTSMjIyE1Pp9P2dnZds14Q0ND6uvrC1kAAMD0FfankyoqKhQMBnXTTTcpJiZGo6Ojevrpp/X9739fkhQIBCRJHo8nZD+Px6MTJ07YNXFxcZo5c+aEmi/2H6+6ulo/+clPwn06AAAgSoV9JubVV1/Vrl27tHv3brW3t+uVV17Rz372M73yyishdQ6HI2TdsqwJY+NdqKayslLBYNBeurq6vtyJAACAqBb2mZgNGzZo06ZNuvfeeyVJOTk5OnHihKqrq7VixQp5vV5Jn8+2XHfddfZ+PT099uyM1+vV8PCwent7Q2Zjenp6lJ+ff85/1+VyyeVyhft0AABAlAr7TMzZs2d1zTWhh42JibEfsc7MzJTX61VjY6O9fXh4WE1NTXZAyc3NVWxsbEjNqVOndOTIkfOGGAAAcHUJ+0zMsmXL9PTTTysjI0Nz587Vu+++q61bt+rBBx+U9PnXSH6/X1VVVcrKylJWVpaqqqqUkJCg0tJSSZLb7dbKlStVXl6u1NRUpaSkaP369crJyVFBQUG4WwYAAAYKe4h57rnn9OSTT2r16tXq6emRz+fTqlWr9E//9E92zcaNGzU4OKjVq1ert7dXeXl52r9/v5KSkuyabdu2yel0qqSkRIODg1q8eLHq6uoUExMT7pYBAICBwv6emGjBe2KuPN4TA0wPvCcGkRTR98QAAABcCYQYAABgJEIMAAAwEiEGAAAYiRADAACMRIgBAABGIsQAAAAjEWIAAICRCDEAAMBIhBgAAGAkQgwAADASIQYAABiJEAMAAIxEiAEAAEYixAAAACMRYgAAgJEIMQAAwEiEGAAAYCRCDAAAMBIhBgAAGIkQAwAAjESIAQAARiLEAAAAIzkj3QC+HIdjRqRbAAAgIpiJAQAARiLEAAAAIxFiAACAkQgxAADASIQYAABgJEIMAAAwEiEGAAAYiRADAACMRIgBAABGIsQAAAAjEWIAAICRCDEAAMBIhBgAAGCkKQkxH330ke6//36lpqYqISFBX//619XW1mZvtyxLmzdvls/nU3x8vBYtWqSjR4+GHGNoaEhr1qzRrFmzlJiYqOLiYp08eXIq2gUAAAYKe4jp7e3VbbfdptjYWP3617/WH/7wBz3zzDOaMWOGXbNlyxZt3bpVtbW1am1tldfr1ZIlS9Tf32/X+P1+NTQ0qL6+Xs3NzRoYGFBRUZFGR0fD3TIAADCQw7IsK5wH3LRpk/77v/9bb7/99jm3W5Yln88nv9+viooKSZ/Pung8HtXU1GjVqlUKBoOaPXu2du7cqeXLl0uSuru7lZ6err1792rp0qUX7aOvr09ut1vBYFDJycnhO8Eo43DMiHQLAKYZyzod6RZwFZvM53fYZ2Jef/11zZ8/X9/73veUlpamefPm6aWXXrK3d3Z2KhAIqLCw0B5zuVxauHChWlpaJEltbW0aGRkJqfH5fMrOzrZrxhsaGlJfX1/IAgAApq+wh5g///nP2r59u7KysvTGG2/ohz/8odauXat//dd/lSQFAgFJksfjCdnP4/HY2wKBgOLi4jRz5szz1oxXXV0tt9ttL+np6eE+NQAAEEXCHmLGxsb0jW98Q1VVVZo3b55WrVqlhx56SNu3bw+pczgcIeuWZU0YG+9CNZWVlQoGg/bS1dX15U4EAABEtbCHmOuuu04333xzyNicOXP04YcfSpK8Xq8kTZhR6enpsWdnvF6vhoeH1dvbe96a8Vwul5KTk0MWAAAwfYU9xNx22216//33Q8aOHz+u66+/XpKUmZkpr9erxsZGe/vw8LCampqUn58vScrNzVVsbGxIzalTp3TkyBG7BgAAXN2c4T7g448/rvz8fFVVVamkpESHDx/Wjh07tGPHDkmff43k9/tVVVWlrKwsZWVlqaqqSgkJCSotLZUkud1urVy5UuXl5UpNTVVKSorWr1+vnJwcFRQUhLtlAABgoLCHmG9+85tqaGhQZWWlnnrqKWVmZurZZ5/VfffdZ9ds3LhRg4ODWr16tXp7e5WXl6f9+/crKSnJrtm2bZucTqdKSko0ODioxYsXq66uTjExMeFuGQAAGCjs74mJFrwnBgAuD++JQSRF9D0xAAAAVwIhBgAAGIkQAwAAjESIAQAARiLEAAAAIxFiAACAkQgxAADASIQYAABgJEIMAAAwEiEGAAAYiRADAACMRIgBAABGIsQAAAAjEWIAAICRCDEAAMBIhBgAAGAkQgwAADASIQYAABiJEAMAAIxEiAEAAEYixAAAACMRYgAAgJEIMQAAwEiEGAAAYCRCDAAAMBIhBgAAGIkQAwAAjESIAQAARiLEAAAAIxFiAACAkQgxAADASM5INwAAiC4Ox4xIt2CzrNORbgFRjJkYAABgJEIMAAAwEiEGAAAYiRADAACMNOUhprq6Wg6HQ36/3x6zLEubN2+Wz+dTfHy8Fi1apKNHj4bsNzQ0pDVr1mjWrFlKTExUcXGxTp48OdXtAgAAQ0xpiGltbdWOHTv0ta99LWR8y5Yt2rp1q2pra9Xa2iqv16slS5aov7/frvH7/WpoaFB9fb2am5s1MDCgoqIijY6OTmXLAADAEFMWYgYGBnTffffppZde0syZM+1xy7L07LPP6oknntB3vvMdZWdn65VXXtHZs2e1e/duSVIwGNTLL7+sZ555RgUFBZo3b5527dql9957T2+++eZUtQwAAAwyZSHmkUce0d13362CgoKQ8c7OTgUCARUWFtpjLpdLCxcuVEtLiySpra1NIyMjITU+n0/Z2dl2DQAAuLpNycvu6uvr1d7ertbW1gnbAoGAJMnj8YSMezwenThxwq6Ji4sLmcH5ouaL/ccbGhrS0NCQvd7X1/elzgEAAES3sM/EdHV16bHHHtOuXbt07bXXnrfO4XCErFuWNWFsvAvVVFdXy+1220t6evrkmwcAAMYIe4hpa2tTT0+PcnNz5XQ65XQ61dTUpF/84hdyOp32DMz4GZWenh57m9fr1fDwsHp7e89bM15lZaWCwaC9dHV1hfvUAABAFAl7iFm8eLHee+89dXR02Mv8+fN13333qaOjQzfccIO8Xq8aGxvtfYaHh9XU1KT8/HxJUm5urmJjY0NqTp06pSNHjtg147lcLiUnJ4csAABg+gr7PTFJSUnKzs4OGUtMTFRqaqo97vf7VVVVpaysLGVlZamqqkoJCQkqLS2VJLndbq1cuVLl5eVKTU1VSkqK1q9fr5ycnAk3CgMAgKtTRH7FeuPGjRocHNTq1avV29urvLw87d+/X0lJSXbNtm3b5HQ6VVJSosHBQS1evFh1dXWKiYmJRMsAACDKOCzLsiLdxFTo6+uT2+1WMBic1l8tORwzIt0CAEwZyzod6RZwhU3m85vfTgIAAEYixAAAACMRYgAAgJEIMQAAwEiEGAAAYCRCDAAAMBIhBgAAGIkQAwAAjESIAQAARiLEAAAAIxFiAACAkQgxAADASIQYAABgJEIMAAAwEiEGAAAYiRADAACMRIgBAABGIsQAAAAjEWIAAICRCDEAAMBIzkg3YCqHY0akWwAA4KrGTAwAADASIQYAABiJEAMAAIxEiAEAAEYixAAAACMRYgAAgJEIMQAAwEi8JwYAELWi5Z1clnU60i3gHJiJAQAARiLEAAAAIxFiAACAkQgxAADASIQYAABgJEIMAAAwEiEGAAAYiRADAACMFPYQU11drW9+85tKSkpSWlqa7rnnHr3//vshNZZlafPmzfL5fIqPj9eiRYt09OjRkJqhoSGtWbNGs2bNUmJiooqLi3Xy5MlwtwsAAAwV9hDT1NSkRx55RAcPHlRjY6M+++wzFRYW6syZM3bNli1btHXrVtXW1qq1tVVer1dLlixRf3+/XeP3+9XQ0KD6+no1NzdrYGBARUVFGh0dDXfLAADAQA7Lsqyp/Ac++eQTpaWlqampSd/61rdkWZZ8Pp/8fr8qKiokfT7r4vF4VFNTo1WrVikYDGr27NnauXOnli9fLknq7u5Wenq69u7dq6VLl1703+3r65Pb7VYwGFRycnLYzytaXoUNAJh6/OzAlTOZz+8pvycmGAxKklJSUiRJnZ2dCgQCKiwstGtcLpcWLlyolpYWSVJbW5tGRkZCanw+n7Kzs+2a8YaGhtTX1xeyAACA6WtKQ4xlWVq3bp1uv/12ZWdnS5ICgYAkyePxhNR6PB57WyAQUFxcnGbOnHnemvGqq6vldrvtJT09PdynAwAAosiUhphHH31Uv//97/Vv//ZvE7Y5HI6QdcuyJoyNd6GayspKBYNBe+nq6rr8xgEAQNSbshCzZs0avf766zpw4IC+8pWv2ONer1eSJsyo9PT02LMzXq9Xw8PD6u3tPW/NeC6XS8nJySELAACYvsIeYizL0qOPPqrXXntN//mf/6nMzMyQ7ZmZmfJ6vWpsbLTHhoeH1dTUpPz8fElSbm6uYmNjQ2pOnTqlI0eO2DUAAODq5gz3AR955BHt3r1b//7v/66kpCR7xsXtdis+Pl4Oh0N+v19VVVXKyspSVlaWqqqqlJCQoNLSUrt25cqVKi8vV2pqqlJSUrR+/Xrl5OSooKAg3C0DAAADhT3EbN++XZK0aNGikPF/+Zd/0T/8wz9IkjZu3KjBwUGtXr1avb29ysvL0/79+5WUlGTXb9u2TU6nUyUlJRocHNTixYtVV1enmJiYcLcMAMAFRdNrNXjc+/9N+XtiIoX3xAAApqPpHmKi6j0xAAAAU4EQAwAAjESIAQAARiLEAAAAIxFiAACAkQgxAADASIQYAABgJEIMAAAwEiEGAAAYiRADAACMRIgBAABGIsQAAAAjEWIAAICRCDEAAMBIhBgAAGAkQgwAADASIQYAABiJEAMAAIxEiAEAAEYixAAAACMRYgAAgJEIMQAAwEiEGAAAYCRCDAAAMBIhBgAAGIkQAwAAjOSMdAMAAODSORwzIt2CzbJOR/TfZyYGAAAYiRADAACMRIgBAABGIsQAAAAjEWIAAICRCDEAAMBIhBgAAGAkQgwAADASIQYAABiJEAMAAIwU9SHm+eefV2Zmpq699lrl5ubq7bffjnRLAAAgCkR1iHn11Vfl9/v1xBNP6N1339Xf/d3f6a677tKHH34Y6dYAAECEOSzLsiLdxPnk5eXpG9/4hrZv326PzZkzR/fcc4+qq6svuG9fX5/cbreCwaCSk5PD3ls0/QAXAACRMBU/ADmZz++o/RXr4eFhtbW1adOmTSHjhYWFamlpmVA/NDSkoaEhez0YDEr6/GJMjajNfgAAXBFT8Rn7xTEvZY4lakPM//zP/2h0dFQejydk3OPxKBAITKivrq7WT37ykwnj6enpU9YjAABXM7fbPWXH7u/vv+jxozbEfMHhcISsW5Y1YUySKisrtW7dOnt9bGxMf/nLX5SamnrO+ummr69P6enp6urqmpKvz6YTrtXkcL0uHdfq0nGtLt3Vdq0sy1J/f798Pt9Fa6M2xMyaNUsxMTETZl16enomzM5IksvlksvlChmbMWPGVLYYlZKTk6+KP/Jw4FpNDtfr0nGtLh3X6tJdTdfqUmd4ovbppLi4OOXm5qqxsTFkvLGxUfn5+RHqCgAARIuonYmRpHXr1qmsrEzz58/XggULtGPHDn344Yf64Q9/GOnWAABAhEV1iFm+fLn+93//V0899ZROnTql7Oxs7d27V9dff32kW4s6LpdLP/7xjyd8pYaJuFaTw/W6dFyrS8e1unRcq/OL6vfEAAAAnE/U3hMDAABwIYQYAABgJEIMAAAwEiEGAAAYiRAzzVRXV8vhcMjv90e6laj00Ucf6f7771dqaqoSEhL09a9/XW1tbZFuK+p89tln+tGPfqTMzEzFx8frhhtu0FNPPaWxsbFItxZxb731lpYtWyafzyeHw6E9e/aEbLcsS5s3b5bP51N8fLwWLVqko0ePRqbZKHCh6zUyMqKKigrl5OQoMTFRPp9PDzzwgLq7uyPXcARd7G/rr61atUoOh0PPPvvsFesvGhFippHW1lbt2LFDX/va1yLdSlTq7e3VbbfdptjYWP3617/WH/7wBz3zzDNX5ZudL6ampkYvvPCCamtrdezYMW3ZskU//elP9dxzz0W6tYg7c+aMbrnlFtXW1p5z+5YtW7R161bV1taqtbVVXq9XS5YsUX9//xXuNDpc6HqdPXtW7e3tevLJJ9Xe3q7XXntNx48fV3FxcQQ6jbyL/W19Yc+ePTp06NAlvZZ/2rMwLfT391tZWVlWY2OjtXDhQuuxxx6LdEtRp6Kiwrr99tsj3YYR7r77buvBBx8MGfvOd75j3X///RHqKDpJshoaGuz1sbExy+v1Wv/8z/9sj3366aeW2+22XnjhhQh0GF3GX69zOXz4sCXJOnHixJVpKkqd71qdPHnS+pu/+RvryJEj1vXXX29t27btivcWTZiJmSYeeeQR3X333SooKIh0K1Hr9ddf1/z58/W9731PaWlpmjdvnl566aVItxWVbr/9dv3mN7/R8ePHJUm/+93v1NzcrG9/+9sR7iy6dXZ2KhAIqLCw0B5zuVxauHChWlpaItiZOYLBoBwOBzOk5zA2NqaysjJt2LBBc+fOjXQ7USGq39iLS1NfX6/29na1trZGupWo9uc//1nbt2/XunXr9I//+I86fPiw1q5dK5fLpQceeCDS7UWViooKBYNB3XTTTYqJidHo6Kiefvppff/73490a1Htix+sHf8jtR6PRydOnIhES0b59NNPtWnTJpWWll41P3Q4GTU1NXI6nVq7dm2kW4kahBjDdXV16bHHHtP+/ft17bXXRrqdqDY2Nqb58+erqqpKkjRv3jwdPXpU27dvJ8SM8+qrr2rXrl3avXu35s6dq46ODvn9fvl8Pq1YsSLS7UU9h8MRsm5Z1oQxhBoZGdG9996rsbExPf/885FuJ+q0tbXp5z//udrb2/lb+it8nWS4trY29fT0KDc3V06nU06nU01NTfrFL34hp9Op0dHRSLcYNa677jrdfPPNIWNz5szRhx9+GKGOoteGDRu0adMm3XvvvcrJyVFZWZkef/xxVVdXR7q1qOb1eiX9/4zMF3p6eibMzuD/jYyMqKSkRJ2dnWpsbGQW5hzefvtt9fT0KCMjw/7/+hMnTqi8vFxf/epXI91exDATY7jFixfrvffeCxn7wQ9+oJtuukkVFRWKiYmJUGfR57bbbtP7778fMnb8+HF+UPQczp49q2uuCf1vnJiYGB6xvojMzEx5vV41NjZq3rx5kqTh4WE1NTWppqYmwt1Fpy8CzJ/+9CcdOHBAqampkW4pKpWVlU2453Hp0qUqKyvTD37wgwh1FXmEGMMlJSUpOzs7ZCwxMVGpqakTxq92jz/+uPLz81VVVaWSkhIdPnxYO3bs0I4dOyLdWtRZtmyZnn76aWVkZGju3Ll69913tXXrVj344IORbi3iBgYG9MEHH9jrnZ2d6ujoUEpKijIyMuT3+1VVVaWsrCxlZWWpqqpKCQkJKi0tjWDXkXOh6+Xz+fTd735X7e3t+tWvfqXR0VF7FislJUVxcXGRajsiLva3NT7gxcbGyuv16sYbb7zSrUaPSD8ehfDjEevz+4//+A8rOzvbcrlc1k033WTt2LEj0i1Fpb6+Puuxxx6zMjIyrGuvvda64YYbrCeeeMIaGhqKdGsRd+DAAUvShGXFihWWZX3+mPWPf/xjy+v1Wi6Xy/rWt75lvffee5FtOoIudL06OzvPuU2SdeDAgUi3fsVd7G9rPB6xtiyHZVnWFU1NAAAAYcCNvQAAwEiEGAAAYCRCDAAAMBIhBgAAGIkQAwAAjESIAQAARiLEAAAAIxFiAACAkQgxAADASIQYAABgJEIMAAAwEiEGAAAY6f8A23w45ge+su8AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Средний доход за эпизод равен:', np.mean(env.return_queue))\n",
    "print('Минимальный доход за эпизод равен:', np.min(env.return_queue))\n",
    "print('Максимальный доход за эпизод равен:', np.max(env.return_queue))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(env.return_queue, bins=13, color='#00000f')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно отметить, что минимальный доход за эпизод равен 3, это соответствует наихудшему начальному состоянию, когда такси требуется проехать максимальное число клеток.\n",
    "\n",
    "Можно также найти наилучший средний доход за 100 эпизодов, сделав несколько прогонов этих 100 эпизодов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.54\n",
      "7.75\n",
      "8.57\n"
     ]
    }
   ],
   "source": [
    "best = 0\n",
    "for i in np.arange(100):\n",
    "    gains = play(env, 100, optPolicy)\n",
    "    avg = np.mean(gains)\n",
    "    if avg>best:\n",
    "        best = avg\n",
    "        print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMDnfQdfW/8Vf/y6aqJb9pK",
   "name": "2.1 RL_DP_netWorld.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
